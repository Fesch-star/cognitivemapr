---
title: "Analysing Cognitive Maps with cognitivemapr"
author: 
- name: Femke Van Esch
  affiliation: Utrecht University School of Governance
  email: F.a.w.j.vanesch@uu.nl
- name: Jeroen Snellens
  affiliation: Independent researcher
date: "2023-04-16"
output:
  pdf_document: default
  html_document: default
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction

This article will introduce the method of Cognitive Mapping (CM) as well
as the Rpackage cognitivemapr (still under development) that will reduce
the initial investment needed to start using the method. CM is a method
specifically designed to study belief systems as espoused by individuals
or organisations and has several advantages over other methods.It starts
from the premise that belief systems are best conceived as semantic
networks consisting of causal and normative relations between ideas
(Axelrod 1976; Young 1996; Yang and González-Bailón 2017). Using
visualisation techniques as well as graph theory, CM fosters a more in
depth understanding of beliefs than other methods as it helps reveal the
argumentation underlying belief systems, makes it possible to
distinguish amongst different types of beliefs (core versus peripherical
beliefs, instrumental versus paradigmatic beliefs) as well as establish
the saliency of beliefs and argumentations (Van Esch and Snellens online
first). Moreover, it allows scholars to study the belief systems of
political actors in both a qualitative and quantitative, deductive and
inductive manner. Finally, the CM technique can be used to study beliefs
embedded in texts or speech acts as well as be used as a survey tool. It
is therefore suitable to study the beliefs of political actors and
citizens in an indirect way (at-a-distance) or by querying respondents
directly (Van Esch, Joosen, and Van Zuydam 2016). To illustrate the
various ways in which the technique of Cognitive Mapping may be used,
the paper will draw upon a database of cognitive maps of **European
political and financial leaders and citizens from nine EU member states
regarding the Eurozone crisis.**

# Cognitive Mapping

Cognitive mapping offers scholars a standardised way of studying ideas
and has been successfully applied in the fields of political and social
psychology, communication and organizational sciences and economics
(Axelrod 1976; Bougon, Weick, and Binkhorst 1977; Boukes et al. 2020;
Laukkanen and Wang 2016; Van Esch 2012). The CM technique rests upon the
premise that ideas are reflected in spoken or written communication like
speech acts, institutional documents, media sources and interviews.
Moreover, the technique has also been used as a tool to elicit the
beliefs of individuals directly during interviews or in survey settings.
Finally, the method has been used in the context of focus groups to
elicit, study or stimulate the development of collective belief systems
(Boukes et al; Van Esch, Joosen, and Van Zuydam 2016; Curseu et al
2010). Whereas other techniques to derive ideas from text use concepts
or themes as their unit of analysis, the basis of a cognitive map is the
relationship between the concepts (Axelrod 1976). More specifically, to
create a cognitive map all the causal and utility relationships between
concepts are derived from a text or elicited from respondents. Utility
relations are statements to the effect that concept x is 'good', 'in
someone's interest' or 'in the general benefit'. Conducting a CM
analysis thus reveals actors' arguments (a leads to b) as well as their
normative evaluations (b is a worthy cause / beneficial), and as such
provides an in-depth analysis of the beliefs and underlying
argumentation embedded in texts or speech acts. In contrast to other
qualitative text analyses, the annotation of texts or direct elicitation
of responses for the CM technique is generic in the sense that it is
conducted in the same manner regardless of the topic at hand. For the CM
as a text-analysis method, detailed coding-manuals exist, and earlier
studies indicate that scholars need relatively little training to
achieve good rates of inter-coder reliability (Axelrod 1976; Hart 1977;
Young and Schafer 1998). In addition, scholars have also researched and
discussed best practices when using the direct elicitation form of CM
analysis and do's and don't when integrating the method in a survey
(refs). The method therefore forms a reliable alternative to other
methods used to study ideas and beliefs, and due to its generic and
structured nature may facilitate comparison across studies and knowledge
accumulation. Another added value of cognitive mapping is that the
relationships between concepts represented in a CM may be analysed in a
qualitative and quantitative manner, making the technique useful for a
wide variety of research questions. To facilitate the coding and
analysis of cognitive maps various tool and applications are available,
but requires a steep learning curve and initial investment (Bastian,
Heymann, and Jacomy 2009; Young 1996). The newly developed R-package
cognitivemapr bundles visual, qualitative and quantitative modes of
analyses.

## Gathering the data

As indicated above, cognitive map data may be obtained in two ways:
Through indirect elicitation throught the analysis of texts or
speech-acts or by direct elicitation of the data from respondents
through a survey or focus groups. The first method is common for studies
that focus on political elite, whereas the second is mostly used when
scholars are interested in the beliefs of the public of members of an
organisation.

The indirect method of eliciting CM data rests upon the premise that
ideas are reflected in spoken or written communication like speech acts,
institutional documents, media sources and interviews. Whereas other
techniques to derive ideas from text use concepts or themes as their
unit of analysis, the basis of a cognitive map is the relationship
between the concepts (Axelrod, 1976). More specifically, to create a
cognitive map all the causal and utility relationships between concepts
are derived from a text. Utility relations are statements to the effect
that concept x is 'good', 'in someone's interest' or 'in the general
benefit'. Larger (sets of) maps may benefit from a certain level of
standardisation of concepts once all relations are derived. Detailed
coding-manuals exist to guide the coding process, and earlier studies
indicate that scholars need relatively little training to achieve good
rates of intercoder reliability (Axelrod, 1976; Hart, 1977; Young &
Schafer, 1998). While some attempts have been made to automate the CM
indirect coding process (Van Esch et al 2022; Young 1992), no viable
automated scheme is available yet.

In the case of indirect elicitation of a cognitive map, two different
procedures are available: the pairwise comparison and freehand drawing
approach. The pairwise comparison presents actors' (leaders or citizens)
with a set of concepts and asks them to evaluate whether pairs of
concepts are causally or normatively related or not. Participants to the
research will have to review all possible combinations of the concepts
in the set (Hodgkinson et al 2004; Clarkson & Hodgkinson 2005; Markiczy
& Goldberg 1995). Previous studies suggest that this method results in
very comprehensive maps, but the method is also more time-consuming and
labour intensive and is seen to be tedious by participants. In addition,
Hodgkinson et all (2004) have found that participants in their study
rate representativeness of the resulting maps as much lower than of
those resulting from the freehand approach. With the freehand approach,
respondents start by choosing concepts they deem most relevant for the
topic at hand (from a predetermined list, or inductively) and
graphically draw arrows between these concepts to represent their ideas
on how these concepts relate. Participants draw different types of
arrows to represent negative or positive relationships between concepts.
This results in a cognitive map that reflects the ideas of the
respondents directly.[^1]

[^1]: In order to allow the participant to draw maps directly, the
    authors developed aweb-based application was developed, DART. This
    software allows respondents to draw their personal map freehand
    through a user-friendly digital interface and automatically creates
    a digital database of cognitive maps. The data from these maps can
    be linked to the outcomes of survey questions about political and
    demographic characteristics of respondents, and allows for the
    aggregation, processing and analysis of data from large groups of
    citizens. Finally, DART exports the survey and CM data in the
    formats required by other analytical (CM) software packages like
    SPSS, Worldview and Gephi (Bastian et al 2009; Young 1996).

To analyse this data, various quantitative measures are used. Moreover,
to facilitate qualitative or narrative analysis, the data is often
transformed into a visual graph - the cognitive map - in which the
derived concepts are depicted as points and the relations between these
concepts as arrows (Axelrod 1976; Young 1996; Young and Schafer 1998,
see **figure** **1**). Various tool and applications are available to
analyse cognitive maps, but few offer an open-source full workflow or
produce CMs that can readily be used for qualitative analysis, several
also require a steep learning curve (Bastian, Heymann, and Jacomy 2009;
Young 1996). By developing the R-package cognitivemapr, we hope to
bundle visual, qualitative and quantitative modes of analyses to lower
the threshold for scholars to start using cognitive maps in their
research.

## Preparing the data

The cognitivemapr package requires you to compile your CM data into two
csv.files with several mandatory variables included. In addition the
exact order of the mandatory columns should be maintained (except for
the value.x and value.y in the edgelist).

1.  An edgelist with all the relations in a cognitive map, with the
    following mandatory columns:

    -   "from": containing the id of "cause"-concept

    -   "to": containing the id of the "effect"-concept

    -   "edge_value": whether the relation is positive, negative or
        non-existent

    -   "weight": the number of times the (exact same) relation appears
        in your data

    -   "value.x": the value of the cause ('from') concept: -1 =
        negative, 0 = ambiguous, 1 = positive (see nodelist)

    -   "value.y": the value of the effect ('to') concept: -1 =
        negative, 0 = ambiguous, 1 = positive (see nodelist)

2.  A nodelist with all the unique concepts a cogntive map, with the
    following mandatory columns:

    -   "id": the id of the concept

    -   "value": whether the concept is inherently negative (-1),
        positive (1) or ambiguous (0)[^2]
        
***ADD INSTRUCTION ON HOW TO PREP DATA FOR PARADIGM & CATEGORY CALCS***

[^2]: This should be consistent to the values (value.x / value.y) in the
    edgelist. We would advise to be reticent in assigning concepts a
    negative pr ambiguous value when working with a set of maps
    especially when these involve different actors as the concept will
    be seen as see to be inherently negative/ambiguous by all actors.
    Moreover, one of the strengths of the CM analysis is that it offers
    ways to derive the value of concepts per CM (see section on
    Evaluation below).

On top of these mandatory variables, you may want to add additional
information like meta-data (name of actor, date), the name of the
concepts or some kind of categorisation of the concepts for practical or
theoretical reasons. In this paper, we will use data concerning the
beliefs of the Dutch prime minister Mark Rutte regarding the Eurozone
crisis as an example. You can download this data from Github by running
the following code:

```{r, results='hide', warning=FALSE, message=FALSE}
library(readr)

load("../data/rutte_p2_edgelist.rda")
load("../data/rutte_p2_nodelist.rda")


```


The top 6 rows of both the edgelist and nodelist are shown below. In
addition to the mandatory columns, it shows the metadata that was
included in the edgelist. Moreover, the head of the nodelist indicates
that we also added the concept names (node_name) to facilitate
qualitative analysis and categorised the concepts in terms of economic
paradigma (eco), type of integration (int) and type of policy instrument
(instr) to conduct some theoretically interesting analyses on the data
later on in this paper (see section on categorisation below).[^3]

[^3]: See Van Esch and Snellens first online for the theoretical
    background of these categorisations.

```{r}
head(rutte_p2_edgelist)
head(rutte_p2_nodelist)
```

## Calculating basic CM measures

There are various ways of analysing CM data contained in the edge and
nodelist. While the full potential of the method lies in transforming
the causal and utility relations derived from text or respondents into a
graph, we will start by calculating some basic quantitative
measures.[^4]

[^4]: As is shown in the calc_degree_goW function below, the calculation
    of these measures actually do require you to transform the edge and
    nodelist into a CM/graph (by using the 'graph_from_data_frame'
    function from the package igraph). However, as a lot more is
    involved to derive a visual CM that can be analysed qualitatively
    and some of the basic quantitative measures can help enhance the
    depth of information included in such visualization (see
    visualisation section below), we will start with the basic
    quantitative measures.

The basis for the analysis of CM data are the weight of relations and
the centrality (C) and saliency (S) of the concepts. The 'weight' of a
relation is determined by how many times an actor refers to the relation
in their texts or speech-acts and is included in the original edgelist
(see the column 3 in the edgelist above). The 'centrality' of a concept
is determined by establishing the number of connections of a concept
with other concepts in the map while the measure 'saliency' denotes the
number of connection of a concept with other concepts, but also takes
the weight of the relations into account. Whereas centrality is used in
CM analysis as a basic measure for the complexity of ideas, the weight
and saliency are indicators of how strong a particular belief is. As
complexity and strength are regarded as consequential for the effect of
ideas and beliefs on political behaviour and policy-making, these
measures may help answer important research questions in the field. In
addition, on the basis of how many relations feed into a concept
(indegree or the weighted variant weighted indegree) and how many
relations feed out from a concept (outdegree or the weighted variant
weighted outdegree), the measure 'goal-orientation (go)' (or its
weighted variant, gow) can be calculated. This measure provides an
indication of the extent to which a concept is seen as a cause (go(w) =
-1) or an effect (go(w) = 1) in the CM that is being analysed.

To calculate these basic measures, the cognitivemapr package uses
functions from the igraph packages. Although this works well, the igraph
package uses different terms for several of the measures (see table
**x** below).

| CM term    | Meaning                                                       | Synonym         |
|------------|---------------------------------------------------------------|-----------------|
| Concept    | Cause or effect (raw or standardised                          | Node            |
| Relation   | Causal or utility relation                                    | Edge            |
| CM         | Visual graph showing concepts and relations                   | Map             |
| Centrality | Number of connections of a concept to other concepts          | Degree          |
| Saliency   | Weighted number of connections of a concept to other concepts | Weighted degree |

In order to calculate all the basic measures in one go and store them
into a dataframe and combine them with the original information on the
concepts, we created the following function 'calc_degrees_goW'.

```{r, message=FALSE, warning=FALSE}
library(igraph)
library(dplyr)

#Creating a function to do the basic CM calculations and store them in a dataframe
calc_degrees_goW <- function(edgelist, nodelist) {

#transform edge & nodelist into a map
  map <- graph_from_data_frame(d = edgelist, vertices = nodelist, directed = TRUE)

#calculate for each node
  deg <- degree(map, mode = "all") #degrees (centrality in CM speech)
  indeg <- degree(map, mode = "in") #indegrees
  outdeg <- degree(map, mode = "out") #outdegrees
  w_indeg <- strength(map, mode = "in") #weighted indegrees 
  w_outdeg <- strength(map, mode = "out") #weighted outdegrees
  w_deg <- strength(map, mode = "all") #weighted degrees (saliency in CM speech)
  
#make new df to store the calculated values
  node_calc <- nodelist

#link vectors with all the (weighted) degrees values to 
#the new node_calc df as columns
  node_calc$indegree <- indeg
  node_calc$outdegree <- outdeg
  node_calc$degree <- deg
  node_calc$w_indegree <- w_indeg
  node_calc$w_outdegree <- w_outdeg
  node_calc$w_degree <- w_deg
  
#calculates go & goW and link it to the df node_calc as columns
  node_calc <- mutate(node_calc, 
          go = (node_calc$indegree - node_calc$outdegree) / node_calc$degree,
          gow = (node_calc$w_indegree - node_calc$w_outdegree) / node_calc$w_degree)
  return(node_calc) #returns the df node_calc with all calculated values
}

```

Running this function using the data of Rutte shows that the function
returns a dataframe in which all the original data on the nodes in the
CM is combined with the basic measures. Running the summary shows some
basic characterising and statistical information regarding the variables
in the dataframe. This provides some first indication regarding the
number of concepts in the CM, the difference in strenght of the concepts
in the map (minimum, maximum, mean w_degree) as well as the overall
complexity of the map (mean degree) that may help compare the CM to
others.

```{r}
#running the function with the data of Mark Rutte, and storing it as a df
rutte_p2_node_calc <- calc_degrees_goW(rutte_p2_edgelist, rutte_p2_nodelist)

#provide summary statistics for all measures
summary(rutte_p2_node_calc)

```

In addition, the sum of saliency tells us of how many relations the CM
consists, which is the most commonly used measure of the relative size
of a CM in comparison to others.[^5]

[^5]: The sum of the saliency of all concepts is always twice the amount
    of relations in a CM, as saliency counts both 'sides' of the
    relationship: the weighted indegree and weighted outdegree. When
    used consistently as a measure of map-size dividing it by two is
    superfluous.

```{r}
#Sum of w_degree provides a first indication of the size of the CM 
sum(rutte_p2_node_calc$w_degree)

```

At the concept level, the output of the calc_degrees_goW function also
provides us with the first feel of the content of the CM. The table
below, for instance shows the top 10 concepts in terms of saliency and
economic paradigm (ordoliberal or keynesian) for the map of Rutte. The
table provides a first indication that the Dutch prime minister was
highly concerned about the Eurozone crisis, and was discussing several
institutional and predominantly ordoliberal measures to tackle the
crisis, while also debating the value of being pragmatic.

```{r}
# order the dataframe by saliency
rutte_p2_node_calc <- rutte_p2_node_calc[order(rutte_p2_node_calc$w_degree, decreasing = TRUE),]

rutte_p2_node_calc[1:10,c("id", "node_name", "w_degree", "eco")]

```

## Evaluation of ideas

One of the main strengths of the CM over especially other (automatic or
thematic) text analysis techniques, however, is that CM does not only
allow us to derive the saliency or strength of actors ideas from texts
or speech-acts, but that CM allows us to establish to what extent actors
are positive or negative about specific ideas. This is a direct result
of the fact that CM perceives and represents a belief system as a
semantic network. To establish to what extent an idea in a CM is seen as
positive, negative or ambiguous, the 'paths' in a map are analysed. A
path is a sequence of concepts and relations. Any path leading out of a
concept is called a consequent path, any path feeding into a concept is
an antecedent path. By analysing the relationships between the concepts
in a path, scholars can establish if, and how concepts are peceived to
be related: positively, negatively or ambiguously (Hart 1977). For
instance, assuming that 'solving the crisis' is considered a positive
goal, we can derive from **figure 1** that 'fiscal discipline' is valued
positively as it contributes to 'solving the crisis', whereas 'wider
yield spreads' is negatively evaluated.

While this is a straightforward example, establishing the overall
evaluation of all concepts in a full CM requires walking through all of
the consequent paths emerging from each concept. This may reveal that a
single concept may have both positive and negative effects leading to an
ambiguous overall evaluation of a concept. Naturally, the establishment
of the evaluation of the other concepts in a map should subsequently be
based on this 'new' evaluation of the concept, making the excercise
rather complex. Moreover, not all consequent path end in a concept that
is unambiguously positive or negative, thus requiring case-by-case
interpretation by the researcher. When such interpretation is free of
bias and informed by the researcher's indepth knowlegde of a case, this
may increase the validity of the outcomes.[^6] However, overall it is
clear that conducting the evaluation analysis manually is a cumbersome
and complex process which requires a very clear prior visualisation of
the CM and may increase the risk of introducing mistakes or
interpretation biases. Conducting the analysis automatically is faster,
prevents mistakes and even though it will prevent possibly useful
case-by-case interpretations, it will increase reliability.

[^6]: Indepth understanding of the cases to be studied, may reveal that
    different actors evaluate certain values or policy goals
    differently. While conflict, for instance, will be seen as negative
    in most instances, there may be some cases in which the concept is
    seen as positive (in case of an uprising against autocrats for
    instance). Even when using the automatic evaluation, scholars can
    integrate such case-by-case differences in interpretation by varying
    the predetermined 'value' variable in the original nodelist. The
    data used for this article is derived from a larger data-set in
    which the value of all nodes was kept the same across all actors in
    the set.

To establish the evaluation of all concepts via a holistic iterative
walk through the cognitive map, we should first determine the number of
times we should walk through the CM and adjust the evaluations of the
concepts. To do this, we propose setting the diameter of the CM (the
length to the shortest path between the most distanced concepts) as the
maximum number. This is a somewhat arbitrary choice as some CM are
cyclical and evaluations of concepts may change after each run. At the
same time, the evaluations of all concepts may become stable prior to
the final walk through the CM, making some of the runs superfluous.
However, taking the diameter as the maximum number of runs seems to be a
good compromise as it ensures that all relations in the CM are taken
into account and running additions walks does not take much computing
time for most CMs. To set the maximum number of runs through the CM the
'set_max_runs' function was created.

```{r}
set_max_runs <- function (edgelist, nodelist){

#first draw map to be able to calculate diameter
map <- graph_from_data_frame(d=edgelist, vertices=nodelist, directed = T)

#determine diameter and set it as the maximum of runs for the evaluation
max_runs <- diameter(map, directed = TRUE, unconnected = TRUE) 

return(max_runs)

}
```

Running the function with the original edge and nodelist of Rutte
reveals the maximum number of runs for his CM to be 5.

```{r}
max_runs <- set_max_runs(rutte_p2_edgelist, rutte_p2_nodelist)
```

As may be evident from the description of the process, automatically
determining the evaluation of all concepts in a map taking into account
any changes that may occur in the evaluation of other concepts, requires
quite a complex script. In order to conduct a single run through the map
while adjusting and storing the changing evaluation values of the
concepts, the function 'evaluation_step' was created. The function takes
an edgelist and nodelist as its input (if you want to add the evaluation
to the dataframe with the basic CM measures as calculated above, **be
sure to use the node_calc list**).

```{r}
 # Creating a function to conduct the evaluation analysis
Evaluation_step <- function(edgelist, nodelist) {


  # Conduct the first evaluation calculations
  edgelist$val_xt1 <- edgelist$value.y * edgelist$edge_value # Calculating the value of x at t1 (val_xt1)
  edgelist$val_xt1 <- edgelist$val_xt1 / sqrt(edgelist$val_xt1 ^ 2) # Make val_xt1 unweighted
  edgelist$val_xt1[is.na(edgelist$val_xt1)] <- 0 # Transform NaN values to 0

  # Put the val_xt1 values with their ID in a separate df called xt1
  xt1 <- edgelist[, c("from", "val_xt1")]

  # Add up the xt1 scores per id/concept - then store this in the df xt1
  xt1 <- xt1 %>%
    group_by(from) %>%
    summarise(val_xt1_sum = sum(val_xt1))

  # Make the val_xt values unweighted
  xt1$val_xt1_sum <- xt1$val_xt1_sum / sqrt(xt1$val_xt1_sum ^ 2)

  # Replace NaN by 0
  xt1$val_xt1_sum[is.na(xt1$val_xt1_sum)] <- 0

  # Here the transformation of value.y starts
  # Meaning that you will replace the original value.y with the newly calculated values
  # Rename from=to and add a val_yt1 value that takes the value of val_xt1 as calculated above
  # this is wrong, it reorders the to column

  xt1_to_yt1 <- xt1

  xt1_to_yt1 <- rename(xt1_to_yt1, to = from)

  xt1_to_yt1 <- rename(xt1_to_yt1, val_yt1 = val_xt1_sum)

  # Join this to the original edgelist
  edgelist <- edgelist %>%
    left_join(xt1_to_yt1, by = c("to" = "to"))

  # Replace NA by value.y (so you basically keep the value.y in instances when y
  # only appears as an effect ('to') concept and thus has no new value
  edgelist$val_yt1[is.na(edgelist$val_yt1)] <- edgelist$value.y[is.na(edgelist$val_yt1)]

  # Now start transferring these values to the nodeslist
  # and check if they are consistent over the from/to columns.
  # For x, you need to take the values in column xt1
  # Change the names of the columns accordingly
  xt1 <- rename(xt1, val_run1 = val_xt1_sum)
  xt1 <- rename(xt1, id = from)

  # Take the unique yt1 variables from the table with ids
  yt1 <- unique(edgelist[, c("to", "val_yt1")]) %>%
    rename(val_run1 = val_yt1) %>%
    rename(id = to)

  # Merge xt2 and yt1 while retaining all other columns, collapse the overlap
  node_val_run1 <- merge(xt1, yt1, all = TRUE)

  # Bind node_val_run1 to nodelist
  nodelist <- nodelist %>%
    left_join(node_val_run1, by = "id")

  # Rename some columns to prepare for the next run of the loop
  edgelist$value.x <- edgelist$val_xt1
  edgelist$value.y <- edgelist$val_yt1

  edgelist <- select(edgelist, -c(val_xt1, val_yt1))

  # Return the resulting edge and nodelists with the new values
  return(list(edgelist, nodelist))
}
```


```{r}
# Below the suggestion by chatGPT - it indeed iterates the function and let you look back at the different steps, which is great. But the result is incorrect and does not correspond to running the evaluation_step function manually multiple times.
# I did not manage to solve it.

result_list <- vector("list", max_runs)

for (i in 1:max_runs) {
  # Call the function with appropriate edgelist and nodelist
  # Replace 'your_edgelist' and 'your_nodelist' with the actual data you want to pass to the function
  result_list[[i]] <- Evaluation_step(rutte_p2_edgelist, rutte_p2_node_calc)
}
```

***lijkt ook niet te werken, geen verschil in lijsten per run - als je de functie stap voor stap runt, doet hij het wel***

```{r}
# Access the results for each run
# for (i in 1:max_runs) {
#  edgelist_result <- result_list[[i]][[1]]
#  nodelist_result <- result_list[[i]][[2]]
#}
  
# Perform any additional actions or analysis by drawing out the list you need. 
# In our case the nodelist of the final (5th) run
  
  rutte_p2_node_calc <- result_list[[5]][[2]]
```


A CM analysis thus reveals both the nature, strength and complexity of
ideas as well as whether they are valued positively or negatively.

## Paradigm support (calc_paradigm_support)

In the case of the Eurozone crisis, scholars have identified two
competing paradigms underlying the policy debate: Keynesianism and
Ordoliberalism. The Ordoliberal paradigm is characterised by a belief in
the primacy of price stability which may be ensured by pursuing
austerity and denouncing monetary financing. In contrast, for
Keynesians, economic growth and employment take precedence and economic
stimulation is advocated to promote these goals during economic
downturns. Keynesians are also more favourable to monetary financing and
for a central bank to act as a lender of last resort (Dullien and Guérot
2012; Hall 2014). To capture the paradigmatic orthodoxy of Rutte and
Knot's belief system, all concepts in their CM were classified as either
Keynesian, Ordoliberal or neutral (Van Esch et al. 2018). If a concept associated
with a particular paradigm is evaluated positively,then their saliency scores are
counted towards their score of support for said paradigm. If a concept associated
with a particular paradigm is evaluated negatively,then their saliency scores are
counted towards their score of support for the rivalling paradigm. This way of 
calculating support corresponds to the theoretical idea that paradigms are 
incommensurable (cf. Princen and van Esch 2016; see below for 
a function to use when presuming paradigms are not incommensurable): In our example
case, this mean for instance that a negative evaluation of a Keynesian measure
like economic stimulation would be interpreted as support for the Ordoliberal
paradigm.


```{r}


# This function calculates paradigm support in a zero-sum manner: meaning that a 
# negative evaluation of concepts associated with paradigm a, is interpreted as 
# support for paradigm b and vice versa.
# Function works but needs to be made generic: prescribing generic names for the
# columns in the input data. I propose 'paradigm' rather than eco. Changing this
# requires making a similar change in the test data in the package (rutte_p2_nodelist)

calc_paradigm_support <- function(node_calc, paradigm_a, paradigm_b){

#if a paradigm concept is evaluated positively (> 0), add their w_degree to a new paradigm_a column in the node_calc df'
  node_calc[, paradigm_a] <- case_when (node_calc$eco == paradigm_a &
                                           node_calc$val_run1 > 0 ~ node_calc$w_degree,
                                         #if a paradigm_b concept is evaluated negatively, add their w_degree the paradigm_a column
                                         node_calc$eco == paradigm_b &
                                           node_calc$val_run1 < 0 ~ node_calc$w_degree)
#all other concepts are assigned a zero score in the paradigm_a column by nan <- 0
  node_calc[paradigm_a][is.na(node_calc[paradigm_a])] <- 0

#same process for paradigm_b
  node_calc[, paradigm_b] <- case_when (node_calc$eco == paradigm_b &
                                           node_calc$val_run1 > 0 ~ node_calc$w_degree,
                                         node_calc$eco == paradigm_a &
                                           node_calc$val_run1 < 0 ~ node_calc$w_degree)

  node_calc[paradigm_b][is.na(node_calc[paradigm_b])] <- 0 #nan omzetten in 0

  return <- node_calc

}
```


The result of this analysis shows that during the period from May 2010 and July 2012
both Rutte and Knot's belief system are more Ordoliberal than Keynesian
(see figure 2). Other types of (automated) text analysis are capable of
classifying text into broad categories in a similar fashion (Ban 2015).
The nuance of the CM analyses, however, reveals that there are also
considerable Keynesian elements in Rutte´s and Knot´s belief system,
lending support to the thesis that paradigms are not incommensurable
(Carstensen 2011b; Princen and Van Esch 2016). Moreover, due to the
standardised way the CM technique works, we are also able to compare
their scores to other leaders and conclude that internationally, the
Dutch leaders have a relatively high leaning towards the Ordoliberalism
(Van Esch et al. 2018). In a similar way, we could also analyse changes
in ideas over time, thereby fulfilling the third requirement discussed
above (Bonham, Shapiro, and Trumble 1979; Van Esch 2014).


```{r}
# derive the paradigms from the nodelist
paradigm <- unique(rutte_p2_nodelist$eco)
# if you want to get rid of the non-paradigmatic concepts run:
paradigm <- na.omit(paradigm)
# Now let's run the function for our example
rutte_p2_node_calc <- calc_paradigm_support (rutte_p2_node_calc, "Ordoliberal", "Keynesian")

# Make df to show a barplot with the results
paradigm <- c ("Ordoliberal", "Keynesian")
score <- c(sum(rutte_p2_node_calc$Ordoliberal)/sum(rutte_p2_node_calc$w_degree),
           sum(rutte_p2_node_calc$Keynesian)/sum(rutte_p2_node_calc$w_degree))
paradigm_support <- data.frame (paradigm, score)
ggplot(paradigm_support, aes(x= paradigm, y=score, fill = paradigm, colour = paradigm))+
  geom_bar(stat = "identity")
```

In addition, the CM technique allows scholars to distinguish between
different types of beliefs, For instance, it is also possible to
establish instrumental beliefs with CM. One way of doing this is to
inductively identify a range of possible policy-instruments relevant in
the policy area under study. For this, we used a larger set of cognitive
maps regarding the Eurozone crisis to derive seven relevant policy
measures: Stronger EU fiscal regulation, structural reforms, monetary
measures by the ECB, economic stimulation, fiscal support, financial
market measures and EMU reforms (Van Esch et al. 2018). Comparing Rutte
and Knot's maps shows that while they both lean towards an Ordoliberal
approach, they differ in terms of their instrumental beliefs: Rutte
favours implementing structural reforms and stronger fiscal regulation
over making institutional reforms to EMU, fiscal support and economic
stimulation. He does not discuss financial market measures and ECB
measures (see figure 3). Knot favours interventions by the ECB over
structural and EMU reforms and stronger EU fiscal regulation. In
addition, he shows a similar limited support for providing fiscal aid as
the PM and does not support economic stimulation or financial market
measures. These findings thus allow us to compare the extent the
instrumental ideas of Rutte and Knot influenced the Dutch management of
the Eurozone crisis (see below).

## Causal Power

(HEB ik nog niet omgezet in R code) Finally, taking full advantage of
its graphical nature, CM can be used to establish the causal strength of
the policy instruments identified in a map. For this we combine the
narrative analysis with the quantitative measures. We start by assuming
that the higher the weight of the link between cause and effect, the
stronger actors believe in its causal effect. In addition, we assume
that the larger the distance between the cause and the effect (the more
logical steps it takes to explain the relationship between instrument
and goal), the weaker the presumed causal power of the instrument (cf.
Septer, Dijkstra, and Stokman 2012; Shapiro and Bonham 1973). On the
basis of this, we propose that the causal power (CP) of an instrument on
a particular goal may be established as follows (Septer, Dijkstra, and
Stokman 2012): First, for each subsequent concept in the antecedent
path, its autonomous power (AP) may be determined following the
calculation:

AP = Ev*W*(0.9(D-1)); Whereby: Ev = Evaluation of the cause concept (-1,
0, +1) W = Weight of the relation D = Distance/steps to the effect
concept

To calculate the total causal power of a policy instrument, the
AP-scores of all concepts in the path are multiplied:

CP = AP1 \* AP2 \* ... APi

In figure 1, the causal power of the concept 'ESM' (D=2) runs via (W=2,
EV=+1) the concept 'market trust' (D=1), which in turn positively feeds
into (W=1, EV=+1) 'solving the crisis'. The causal power of ESM is thus
calculated as follows:

CP=(+1*2*(0.9 (2-1)) \* (+1*1*(0.9 (1-1)) = 1.8

Scholars have used similar analyses to derive policy preferences in the
domain of environmental and foreign policy decisions to identify
conditions under which ideas affect policies and even to run simulations
of policy making processes (Bonham, Shapiro, and Trumble 1979; Hart
1976).

## Upgraded visualisation

(draw final map function)

## (nog ergens overlap in maps? )

# Making CM available for scholars without technical skills: the CM shiny app

(HEB ik nog niet omgezet in R code)


# reserve


```{r}
# plot the 10 most salient concepts
plot(rutte_p2_node_calc$w_degree [1:10], rutte_p2_node_calc$gow [1:10], xlab = "Saliency", ylab = "Weighted Goal Orientation", pch =19, frame = FALSE, text(rutte_p2_node_calc$w_degree[1:10], rutte_p2_node_calc$gow[1:10], rutte_p2_node_calc$node_name[1:10], pos = 1, cex = 0.5)) 
```

## Including Plots

You can also embed plots, for example:

```{r pressure, echo=FALSE}
plot(pressure)
```

Note that the `echo = FALSE` parameter was added to the code chunk to
prevent printing of the R code that generated the plot.
